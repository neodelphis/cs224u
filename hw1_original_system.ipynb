{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework original system: Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Pierre Jaumier\"\n",
    "__version__ = \"CS224u, Stanford, Fall 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VSM` = Vector Space-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "from scipy.stats import spearmanr\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import vsm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = [(0.4913448227592214, ['giga5', 'ppmi']),\n",
    "    (0.5048935638458103, ['giga5', 'ppmi', 'lsa', '500']),\n",
    "    (0.4439572029666641, ['giga20', 'ttest']),\n",
    "    (0.514953519371117, ['giga5', 'ppmi', 'ttest', 'lsa', '1000'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb5 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'imdb_window5-scaled.csv.gz'), index_col=0)\n",
    "imdb20 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'imdb_window20-flat.csv.gz'), index_col=0)\n",
    "giga5 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'giga_window5-scaled.csv.gz'), index_col=0)\n",
    "giga20 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'giga_window20-flat.csv.gz'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giga20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dfs = {'imdb5':imdb5, 'imdb20':imdb20, 'giga5':giga5, 'giga20':giga20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result(x):\n",
    "    return \"{:.4f}\".format(x)\n",
    "\n",
    "def macro_average_score(df, parameters):\n",
    "    series = full_word_similarity_evaluation(df)\n",
    "    score = series['Macro-average']\n",
    "    print(format_result(score), '\\t'.join(parameters))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4764 imdb5\tppmi\n",
      "0.3539 imdb20\tppmi\n",
      "0.4913 giga5\tppmi\n",
      "0.4186 giga20\tppmi\n",
      "\n",
      "Meilleure combinaison:\n",
      "\t (0.4913448227592214, ['giga5', 'ppmi'])\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, count_df in count_dfs.items():\n",
    "    # Modèle\n",
    "    df_pmi = vsm.pmi(count_df)\n",
    "    \n",
    "    # Evaluation\n",
    "    series = full_word_similarity_evaluation(df_pmi)\n",
    "    \n",
    "    # Affichage\n",
    "    score = series['Macro-average']\n",
    "    parameters = [name, 'ppmi']\n",
    "    results.append((score, parameters))\n",
    "    print(format_result(score), '\\t'.join(parameters))\n",
    "    \n",
    "print('\\nMeilleure combinaison:\\n\\t',max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4913448227592214, ['giga5', 'ppmi'])\n"
     ]
    }
   ],
   "source": [
    "print(max(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline PPMI - LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3499 imdb5\tppmi\tlsa\t10\n",
      "0.5001 imdb5\tppmi\tlsa\t100\n",
      "0.5013 imdb5\tppmi\tlsa\t500\n",
      "0.4920 imdb5\tppmi\tlsa\t1000\n",
      "0.3030 imdb20\tppmi\tlsa\t10\n",
      "0.4127 imdb20\tppmi\tlsa\t100\n",
      "0.3760 imdb20\tppmi\tlsa\t500\n",
      "0.3565 imdb20\tppmi\tlsa\t1000\n",
      "0.3677 giga5\tppmi\tlsa\t10\n",
      "0.4841 giga5\tppmi\tlsa\t100\n",
      "0.5049 giga5\tppmi\tlsa\t500\n",
      "0.5011 giga5\tppmi\tlsa\t1000\n",
      "0.3465 giga20\tppmi\tlsa\t10\n",
      "0.4184 giga20\tppmi\tlsa\t100\n",
      "0.4262 giga20\tppmi\tlsa\t500\n",
      "0.4235 giga20\tppmi\tlsa\t1000\n",
      "\n",
      "Meilleure combinaison:\n",
      "\t (0.5048935638458103, ['giga5', 'ppmi', 'lsa', '500'])\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, count_df in count_dfs.items():\n",
    "    # Modèle\n",
    "    df_pmi = vsm.pmi(count_df)\n",
    "\n",
    "    for k in [10, 100, 500, 1000]:\n",
    "        df_lsa = vsm.lsa(df_pmi, k)\n",
    "        \n",
    "        # Evaluation\n",
    "        series = full_word_similarity_evaluation(df_lsa)\n",
    "    \n",
    "       # Affichage\n",
    "        score = series['Macro-average']\n",
    "        parameters = [name, 'ppmi', 'lsa', str(k)]\n",
    "        results.append((score, parameters))\n",
    "        print(format_result(score), '\\t'.join(parameters))\n",
    "    \n",
    "print('\\nMeilleure combinaison:\\n\\t',max(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3875 imdb5\tttest\n",
      "0.4087 imdb20\tttest\n",
      "0.3969 giga5\tttest\n",
      "0.4440 giga20\tttest\n",
      "\n",
      "Meilleure combinaison:\n",
      "\t (0.4439572029666641, ['giga20', 'ttest'])\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, count_df in count_dfs.items():\n",
    "    # Modèle\n",
    "    df_ttest = ttest(count_df)\n",
    "    \n",
    "    # Evaluation\n",
    "    series = full_word_similarity_evaluation(df_ttest)\n",
    "    \n",
    "    # Affichage\n",
    "    score = series['Macro-average']\n",
    "    parameters = [name, 'ttest']\n",
    "    results.append((score, parameters))\n",
    "    print(format_result(score), '\\t'.join(parameters))\n",
    "    \n",
    "print('\\nMeilleure combinaison:\\n\\t',max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5049 giga5\tppmi\tlsa\t500\n"
     ]
    }
   ],
   "source": [
    "# Meilleure combinaison:\n",
    "# \t (0.5048935638458103, ['giga5', 'ppmi', 'lsa', '500'])\n",
    "\n",
    "count_df = giga5\n",
    "# Modèle\n",
    "df_pmi = vsm.pmi(count_df)\n",
    "df_lsa = vsm.lsa(df_pmi, k=500)\n",
    "        \n",
    "# Evaluation\n",
    "series = full_word_similarity_evaluation(df_lsa)\n",
    "    \n",
    "# Affichage\n",
    "score = series['Macro-average']\n",
    "parameters = ['giga5', 'ppmi', 'lsa', str(500)]\n",
    "print(format_result(score), '\\t'.join(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3539 giga5\tppmi\tttest\n",
      "0.5134 giga5\tppmi\tttest\tlsa\t500\n"
     ]
    }
   ],
   "source": [
    "count_df = giga5\n",
    "df_pmi = ttest(vsm.pmi(count_df))\n",
    "\n",
    "series = full_word_similarity_evaluation(df_lsa)\n",
    "score = series['Macro-average']\n",
    "parameters = ['giga5', 'ppmi', 'ttest']\n",
    "results.append((score, parameters))\n",
    "print(format_result(score), '\\t'.join(parameters))\n",
    "        \n",
    "for k in [500]:\n",
    "    df_lsa = vsm.lsa(df_pmi, k)\n",
    "\n",
    "    series = full_word_similarity_evaluation(df_lsa)\n",
    "    score = series['Macro-average']\n",
    "    parameters = ['giga5', 'ppmi', 'ttest', 'lsa', str(k)]\n",
    "    results.append((score, parameters))\n",
    "    print(format_result(score), '\\t'.join(parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle ppmi ttest lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4704 imdb5\tppmi\tttest\tlsa\t100\n",
      "0.5085 imdb5\tppmi\tttest\tlsa\t300\n",
      "0.5119 imdb5\tppmi\tttest\tlsa\t500\n",
      "0.5084 imdb5\tppmi\tttest\tlsa\t750\n",
      "0.5089 imdb5\tppmi\tttest\tlsa\t1000\n",
      "0.4262 imdb20\tppmi\tttest\tlsa\t100\n",
      "0.4525 imdb20\tppmi\tttest\tlsa\t300\n",
      "0.4529 imdb20\tppmi\tttest\tlsa\t500\n",
      "0.4496 imdb20\tppmi\tttest\tlsa\t750\n",
      "0.4477 imdb20\tppmi\tttest\tlsa\t1000\n",
      "0.4667 giga5\tppmi\tttest\tlsa\t100\n",
      "0.5056 giga5\tppmi\tttest\tlsa\t300\n",
      "0.5134 giga5\tppmi\tttest\tlsa\t500\n",
      "0.5146 giga5\tppmi\tttest\tlsa\t750\n",
      "0.5150 giga5\tppmi\tttest\tlsa\t1000\n",
      "0.4653 giga20\tppmi\tttest\tlsa\t750\n",
      "0.4645 giga20\tppmi\tttest\tlsa\t1000\n",
      "\n",
      "Meilleure combinaison:\n",
      "\t (0.514953519371117, ['giga5', 'ppmi', 'ttest', 'lsa', '1000'])\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, count_df in count_dfs.items():\n",
    "    # Modèle\n",
    "    df_pmi = vsm.pmi(count_df)\n",
    "    df_ttest = ttest(df_pmi)\n",
    "\n",
    "    for k in [100, 300, 500, 750, 1000]:\n",
    "        try:\n",
    "            df_lsa = vsm.lsa(df_ttest, k)\n",
    "            series = full_word_similarity_evaluation(df_lsa)\n",
    "            score = series['Macro-average']\n",
    "        except:\n",
    "            print(\"Pb de convergence de lsa pour {} et k={}\".format(name, k))\n",
    "            score = 0\n",
    "\n",
    "       # Affichage\n",
    "        parameters = [name, 'ppmi', 'ttest', 'lsa', str(k)]\n",
    "        results.append((score, parameters))\n",
    "        print(format_result(score), '\\t'.join(parameters))\n",
    "    \n",
    "print('\\nMeilleure combinaison:\\n\\t',max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5151 giga5\tppmi\tttest\n",
      "\n",
      "Meilleure combinaison:\n",
      "\t (0.5151364833538356, ['giga5', 'ppmi', 'ttest'])\n"
     ]
    }
   ],
   "source": [
    "count_df = giga5\n",
    "df_pmi = vsm.pmi(count_df)\n",
    "df_ttest = ttest(df_pmi)\n",
    "\n",
    "parameters = ['giga5', 'ppmi', 'ttest']\n",
    "score = macro_average_score(df_ttest, parameters)\n",
    "result = score, parameters\n",
    "print('\\nMeilleure combinaison:\\n\\t', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de glove avec la meilleure configuration  \n",
    "(Le but de glove est d'avoir des vecteurs dont le produit scalaire est proportionnel à la log-probabilité de la co-occurrence)  \n",
    "Renvoie des vecteurs de dim 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depuis le cours vsm_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 226226.6328125"
     ]
    }
   ],
   "source": [
    "from torch_glove import TorchGloVe\n",
    "glove_model = TorchGloVe()\n",
    "imdb5_glv = glove_model.fit(imdb5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3225 imdb5\tglove\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32252879041096516"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_average_score(imdb5_glv, ['imdb5', 'glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrofitting\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/neo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "wn_edges = get_wordnet_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_edges_to_indices(edges, Q):\n",
    "    lookup = dict(zip(Q.index, range(Q.shape[0])))\n",
    "    index_edges = defaultdict(set)\n",
    "    for start, finish_nodes in edges.items():\n",
    "        s = lookup.get(start)\n",
    "        if s:\n",
    "            f = {lookup[n] for n in finish_nodes if n in lookup}\n",
    "            if f:\n",
    "                index_edges[s] = f\n",
    "    return index_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_glove = imdb5_glv\n",
    "wn_index_edges = convert_edges_to_indices(wn_edges, X_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrofitting import Retrofitter\n",
    "wn_retro = Retrofitter(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converged at iteration 8; change was 0.0058 "
     ]
    }
   ],
   "source": [
    "X_retro = wn_retro.fit(X_glove, wn_index_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3488 imdb5\tglove\tretro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3488462883538586"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_average_score(X_retro, ['imdb5', 'glove', 'retro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_autoencoder import TorchAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5151 giga5\tppmi\tttest\tlsa\t1000\n"
     ]
    }
   ],
   "source": [
    "count_df = giga5\n",
    "df_pmi = vsm.pmi(count_df)\n",
    "df_ttest = ttest(df_pmi)\n",
    "df_lsa = vsm.lsa(df_ttest, k=1000)\n",
    "#series = full_word_similarity_evaluation(df_lsa)\n",
    "parameters = ['giga5', 'ppmi', 'ttest', 'lsa', '1000']\n",
    "score = macro_average_score(df_ttest, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 19. Training loss did not improve more than tol=1e-05. Final error is 1.3323446864887956e-05."
     ]
    }
   ],
   "source": [
    "df_ae = TorchAutoencoder(max_iter=1000, hidden_dim=300, eta=0.01).fit(df_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3409 giga5\tppmi\tttest\tlsa\t1000\n"
     ]
    }
   ],
   "source": [
    "parameters = ['giga5', 'ppmi', 'ttest', 'lsa', '1000']\n",
    "score = macro_average_score(df_ae, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est pas vraiment probant..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas facile d'utiliser glove et quand ça marche, on reste toujours bien en deçà des autres méthodes quant aux résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution proposée par la meilleure team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5101 imdb5\tppmi\tttest\n"
     ]
    }
   ],
   "source": [
    "count_df = imdb5\n",
    "df_pmi = vsm.pmi(count_df)\n",
    "df_ttest = ttest(df_pmi)\n",
    "\n",
    "parameters = ['imdb5', 'ppmi', 'ttest']\n",
    "score = macro_average_score(df_ttest, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converged at iteration 3; change was 0.0041 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5646 imdb5\tppmi\tttest\tretro\n"
     ]
    }
   ],
   "source": [
    "wn_retro = Retrofitter(verbose=True)\n",
    "df_retro = wn_retro.fit(df_ttest, wn_index_edges)\n",
    "parameters = ['imdb5', 'ppmi', 'ttest', 'retro']\n",
    "score = macro_average_score(df_retro, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 33. Training loss did not improve more than tol=1e-05. Final error is 0.0005137750194990076."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5782 imdb5\tppmi\tttest\tretro\tl2\tae\n"
     ]
    }
   ],
   "source": [
    "df_retro_l2 = df_retro.apply(vsm.length_norm, axis=1)\n",
    "df_ae = TorchAutoencoder(max_iter=100, hidden_dim=1000, eta=0.001).fit(df_retro_l2)\n",
    "parameters = ['imdb5', 'ppmi', 'ttest', 'retro', 'l2', 'ae']\n",
    "score = macro_average_score(df_ae, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo, en effet c'est mieux. A noter que l'on peut faire du rétrofitting sans passer par glove ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This question asks you to design your own model. You can of course include steps made above (ideally, the above questions informed your system design!), but your model should not be literally identical to any of the above models. Other ideas: retrofitting, autoencoders, GloVe, subword modeling, ... \n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Your code must operate on one or more of the count matrices in `data/vsmdata`. You can choose which subset of them; this is an important design feature of your system. __Other pretrained vectors cannot be introduced__.\n",
    "\n",
    "1. Retrofitting is permitted.\n",
    "\n",
    "1. Your code must be self-contained, so that we can work with your model directly in your homework submission notebook. If your model depends on external data or other resources, please submit a ZIP archive containing these resources along with your submission.\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies. We also ask that you report the best score your system got during development, just to help us understand how systems performed overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSM: retrofitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release two additional datasets. The announcement will go out on the discussion forum. We will also release reader code for these datasets that you can paste into this notebook. You will evaluate your custom model $M$ (from the previous question) on these new datasets using `full_word_similarity_evaluation`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time.\n",
    "\n",
    "The announcement will include the details on where to submit your entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bake-off assessment code into this cell.\n",
    "# Please do not remove this comment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    # Please enter your code in the scope of the above conditional.\n",
    "    ##### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your \"Macro-average\" value as reported by the code above.\n",
    "# Please enter only a number between 0 and 1 inclusive.\n",
    "# Please do not remove this comment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    # Please enter your score in the scope of the above conditional.\n",
    "    ##### YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
