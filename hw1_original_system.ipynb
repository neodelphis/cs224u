{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework original system: Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Pierre Jaumier\"\n",
    "__version__ = \"CS224u, Stanford, Fall 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VSM` = Vector Space-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "from scipy.stats import spearmanr\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import vsm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('computer', 'internet', -7.58)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exbd = wordsim353_reader()\n",
    "next(exbd)\n",
    "next(exbd)\n",
    "next(exbd)\n",
    "next(exbd)\n",
    "next(exbd)\n",
    "next(exbd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb5 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'imdb_window5-scaled.csv.gz'), index_col=0)\n",
    "imdb20 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'imdb_window20-flat.csv.gz'), index_col=0)\n",
    "giga5 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'giga_window5-scaled.csv.gz'), index_col=0)\n",
    "giga20 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'giga_window20-flat.csv.gz'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giga20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dfs = {'imdb5':imdb5, 'imdb20':imdb20, 'giga5':giga5, 'giga20':giga20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb5 \t 0.4763634758188182\n",
      "imdb20 \t 0.3539225011698424\n",
      "giga5 \t 0.4913448227592214\n",
      "giga20 \t 0.4185795186023304\n"
     ]
    }
   ],
   "source": [
    "for name, count_df in count_dfs.items():\n",
    "    df_pmi = vsm.pmi(count_df)\n",
    "    #display(full_word_similarity_evaluation(df_pmi))\n",
    "    series = full_word_similarity_evaluation(df_pmi)\n",
    "    print(df_name, '\\t', series['Macro-average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline PPMI - LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb5 \t 10 \t 0.3499086816105383\n",
      "imdb5 \t 100 \t 0.5001466969819722\n",
      "imdb5 \t 500 \t 0.5012871391776688\n",
      "imdb5 \t 1000 \t 0.49201902661763325\n",
      "imdb20 \t 10 \t 0.3029824559073233\n",
      "imdb20 \t 100 \t 0.41269367060285733\n",
      "imdb20 \t 500 \t 0.3759561261576586\n",
      "imdb20 \t 1000 \t 0.356496703469767\n",
      "giga5 \t 10 \t 0.3676894732477247\n",
      "giga5 \t 100 \t 0.48408884763516785\n",
      "giga5 \t 500 \t 0.5048935638458103\n",
      "giga5 \t 1000 \t 0.501073937148383\n",
      "giga20 \t 10 \t 0.34650895486444905\n",
      "giga20 \t 100 \t 0.4183537479783744\n",
      "giga20 \t 500 \t 0.4262144685093633\n",
      "giga20 \t 1000 \t 0.42349824246101814\n"
     ]
    }
   ],
   "source": [
    "for name, count_df in count_dfs.items():\n",
    "    df_pmi = vsm.pmi(count_df)\n",
    "    for k in [10, 100, 500, 1000]:\n",
    "        df_lsa = vsm.lsa(df_pmi, k)\n",
    "        series = full_word_similarity_evaluation(df_lsa)\n",
    "        print(name, '\\t', k, '\\t', series['Macro-average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb5 \t 0.38750044552433094\n",
      "imdb20 \t 0.4086524279468504\n",
      "giga5 \t 0.39689033014903685\n",
      "giga20 \t 0.4439572029666641\n"
     ]
    }
   ],
   "source": [
    "for name, count_df in count_dfs.items():\n",
    "    df_ttest = ttest(count_df)\n",
    "    series = full_word_similarity_evaluation(df_ttest)\n",
    "    print(name, '\\t', series['Macro-average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gigaword with GloVe [0.5 points]\n",
    "\n",
    "Can GloVe improve over the PPMI-based baselines we explored above? To begin to address this question, let's run GloVe and see how performance on our task changes throughout the optimization process.\n",
    "\n",
    "__Your task__: write a function `run_glove_wordsim_evals` that does the following:\n",
    "\n",
    "1. Has a parameter `n_runs` with default value `5`.\n",
    "\n",
    "1. Reads in `data/vsmdata/giga_window5-scaled.csv.gz`.\n",
    "\n",
    "1. Creates a `TorchGloVe` instance with `warm_start=True`, `max_iter=50`, and all other parameters set to their defaults.\n",
    "\n",
    "1. `n_runs` times, calls `fit` on your model and, after each, runs `full_word_similarity_evaluation` with default keyword parameters, extract the 'Macro-average' score, and add that score to a list.\n",
    "\n",
    "1. Returns the list of scores created.\n",
    "\n",
    "The trend should give you a sense for whether it is worth running GloVe for more iterations.\n",
    "\n",
    "Some implementation notes:\n",
    "\n",
    "* `TorchGloVe` will accept and return `pd.DataFrame` instances, so you shouldn't need to do any type conversions.\n",
    "\n",
    "* Performance will vary a lot for this function, so there is some uncertainty in the testing, but `run_glove_wordsim_evals` will at least check that you wrote a function with the right general logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 62 of 1000; error is 8.9993381500244143"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchGloVe(\n",
      "\tbatch_size=1028,\n",
      "\tmax_iter=1000,\n",
      "\teta=0.001,\n",
      "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
      "\tl2_strength=0,\n",
      "\tgradient_accumulation_steps=1,\n",
      "\tmax_grad_norm=None,\n",
      "\tvalidation_fraction=0.1,\n",
      "\tearly_stopping=False,\n",
      "\tn_iter_no_change=10,\n",
      "\twarm_start=False,\n",
      "\ttol=1e-05,\n",
      "\tembed_dim=2,\n",
      "\talpha=0.75,\n",
      "\txmax=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.22686529159545898"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned vectors:\n",
      "[[2.4587748  0.04361401]\n",
      " [1.3996142  2.9031525 ]\n",
      " [1.2889156  2.0157218 ]\n",
      " [0.627825   1.7479807 ]]\n",
      "We expect the dot product of learned vectors to be proportional to the log co-occurrence probs. Let's see how close we came:\n",
      "Pearson's R: 0.44608053298268946 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44608053298268946"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_glove import simple_example\n",
    "simple_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_glove_wordsim_evals(n_runs=5):\n",
    "\n",
    "    from torch_glove import TorchGloVe\n",
    "    X = pd.read_csv(os.path.join(VSM_HOME, \"giga_window5-scaled.csv.gz\"), index_col=0)\n",
    "    mod = TorchGloVe(warm_start=True, max_iter=50)\n",
    "    results = []\n",
    "    for run in range(n_runs):\n",
    "        G = mod.fit(X)\n",
    "        series = full_word_similarity_evaluation(G)\n",
    "        results.append(series['Macro-average'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_small_glove_evals(data):\n",
    "    \"\"\"`data` should be the return value of `run_glove_wordsim_evals`\"\"\"\n",
    "    assert isinstance(data, list), \\\n",
    "        \"`run_glove_wordsim_evals` should return a list\"\n",
    "    assert all(isinstance(x, float) for x in data), \\\n",
    "        (\"All the values in the list returned by `run_glove_wordsim_evals` \"\n",
    "         \"should be floats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 2587871.18755"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0029026761649535203, 0.006793289047001877, 0.014338311352238975, 0.02657783507573257, 0.04640737043398835]\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    glove_scores = run_glove_wordsim_evals()\n",
    "    print(glove_scores)\n",
    "    test_run_small_glove_evals(glove_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le coefficient de Spearman augmente, donc nos observations sont plus similaires aux résultats souhaités (degrés de similarité en paires de mots)  \n",
    "Par contre on est loin des valeurs obtenues avec ppmi et ppmi_lsa (essai à suivre avec 20 runs)  \n",
    "the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 844641.4531255"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0032487588527864418, 0.006856168461065987, 0.015825893796968547, 0.030936115864081258, 0.05344827882676786, 0.07937041558682159, 0.11107625424218111, 0.15604099781857408, 0.19910988485105555, 0.23421115157698394, 0.2614498660425167, 0.28276753725641324, 0.3009137402769376, 0.31630148172130756, 0.32940416971640607, 0.3401503298980795, 0.34906904587537396, 0.3555732601651972, 0.35991849973249923, 0.3630324058048463]\n"
     ]
    }
   ],
   "source": [
    "glove_scores = run_glove_wordsim_evals(n_runs=20)\n",
    "print(glove_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe9af754090>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(glove_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice coefficient [0.5 points]\n",
    "\n",
    "Implement the Dice coefficient for real-valued vectors, as\n",
    "\n",
    "$$\n",
    "\\textbf{dice}(u, v) = \n",
    "1 - \\frac{\n",
    "  2 \\sum_{i=1}^{n}\\min(u_{i}, v_{i})\n",
    "}{\n",
    "    \\sum_{i=1}^{n} u_{i} + v_{i}\n",
    "}$$\n",
    " \n",
    "You can use `test_dice_implementation` below to check that your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(u, v):\n",
    "    return 1 - 2 * np.sum(np.minimum(u,v)) / np.sum(u + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dice_implementation(func):\n",
    "    \"\"\"`func` should be an implementation of `dice` as defined above.\"\"\"\n",
    "    X = np.array([\n",
    "        [  4.,   4.,   2.,   0.],\n",
    "        [  4.,  61.,   8.,  18.],\n",
    "        [  2.,   8.,  10.,   0.],\n",
    "        [  0.,  18.,   0.,   5.]])\n",
    "    assert func(X[0], X[1]).round(5) == 0.80198\n",
    "    assert func(X[1], X[2]).round(5) == 0.67568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_dice_implementation(dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test reweighting [2 points]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test statistic can be thought of as a reweighting scheme. For a count matrix $X$, row index $i$, and column index $j$:\n",
    "\n",
    "$$\\textbf{ttest}(X, i, j) = \n",
    "\\frac{\n",
    "    P(X, i, j) - \\big(P(X, i, *)P(X, *, j)\\big)\n",
    "}{\n",
    "\\sqrt{(P(X, i, *)P(X, *, j))}\n",
    "}$$\n",
    "\n",
    "where $P(X, i, j)$ is $X_{ij}$ divided by the total values in $X$, $P(X, i, *)$ is the sum of the values in row $i$ of $X$ divided by the total values in $X$, and $P(X, *, j)$ is the sum of the values in column $j$ of $X$ divided by the total values in $X$.\n",
    "\n",
    "For this problem, implement this reweighting scheme. You can use `test_ttest_implementation` below to check that your implementation is correct. You do not need to use this for any evaluations, though we hope you will be curious enough to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " X =  pd.DataFrame(np.array([\n",
    "        [  4.,   4.,   2.,   100.],\n",
    "        [  4.,  61.,   8.,  18.],\n",
    "        [  2.,   8.,  10.,   0.],\n",
    "        [  0.,  18.,   0.,   5.]]))\n",
    "X = X.to_numpy()\n",
    "X_sum = X.sum()\n",
    "P_j = X.sum(axis=0)/X_sum\n",
    "P_i = X.sum(axis=1)/X_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(df):\n",
    "    X = df.to_numpy()\n",
    "    X_sum = X.sum()\n",
    "    P_j = X.sum(axis=0) / X_sum\n",
    "    P_i = X.sum(axis=1) / X_sum\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X[i,j] = (X[i,j] / X_sum - P_i[i] * P_j[j]) / np.sqrt(P_i[i] * P_j[j])\n",
    "    return pd.DataFrame(X, index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ttest_implementation(func):\n",
    "    \"\"\"`func` should be `ttest`\"\"\"\n",
    "    X = pd.DataFrame(np.array([\n",
    "        [  4.,   4.,   2.,   0.],\n",
    "        [  4.,  61.,   8.,  18.],\n",
    "        [  2.,   8.,  10.,   0.],\n",
    "        [  0.,  18.,   0.,   5.]]))\n",
    "    actual = np.array([\n",
    "        [ 0.33056, -0.07689,  0.04321, -0.10532],\n",
    "        [-0.07689,  0.03839, -0.10874,  0.07574],\n",
    "        [ 0.04321, -0.10874,  0.36111, -0.14894],\n",
    "        [-0.10532,  0.07574, -0.14894,  0.05767]])\n",
    "    predicted = func(X)\n",
    "    #print(predicted)\n",
    "    assert np.array_equal(predicted.round(5), actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_ttest_implementation(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsim353         0.543632\n",
       "mturk771           0.604347\n",
       "simverb3500dev     0.205704\n",
       "simverb3500test    0.140476\n",
       "men                0.725626\n",
       "Macro-average      0.443957\n",
       "Name: Spearman r, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### t-test reweighting\n",
    "giga20 = pd.read_csv(os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "ttest_giga20 = ttest(giga20)\n",
    "display(full_word_similarity_evaluation(ttest_giga20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur résultat pour l'instant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching a VSM with subword information [2 points]\n",
    "\n",
    "It might be useful to combine character-level information with word-level information. To help you begin asssessing this idea, this question asks you to write a function that modifies an existing VSM so that the representation for each word $w$ is the element-wise sum of $w$'s original word-level representation with all the representations for the n-grams $w$ contains. \n",
    "\n",
    "The following starter code should help you structure this and clarify the requirements, and a simple test is included below as well.\n",
    "\n",
    "You don't need to write a lot of code; the motivation for this question is that the function you write could have practical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<w>m', 'mo', 'ot', 't</w>']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm.get_character_ngrams('mot', n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bon</th>\n",
       "      <th>bel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bon</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bel</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bon  bel\n",
       "bon    1    2\n",
       "bel    3    4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['bon', 'bel']\n",
    "df = pd.DataFrame([[1, 2], [3, 4]], index=words, columns=words)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bon</th>\n",
       "      <th>bel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;w&gt;b</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n&lt;/w&gt;</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l&lt;/w&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bon  bel\n",
       "<w>b     4    6\n",
       "bo       1    2\n",
       "on       1    2\n",
       "n</w>    1    2\n",
       "be       3    4\n",
       "el       3    4\n",
       "l</w>    3    4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = vsm.ngram_vsm(df, n=2) # Character level VSM\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 12])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm.character_level_rep('bon', cf, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque mot on somme tous les bi-grammes au niveau des caractères  \n",
    "Dans notre exemple cette somme est \"associée\" 10 fois avec `bon` et 16 fois avec `bel`  \n",
    "Les bi-grammes très réccurents dans la matrice vont augmenter le poids du vecteur (ici `<w>b`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_enrichment(df, n=4):\n",
    "    # 1. Use `vsm.ngram_vsm` to create a character-level\n",
    "    # VSM from `df`, using the above parameter `n` to\n",
    "    # set the size of the ngrams.\n",
    "\n",
    "    cf = vsm.ngram_vsm(df, n) # Character level VSM\n",
    "\n",
    "\n",
    "    # 2. Use `vsm.character_level_rep` to get the representation\n",
    "    # for every word in `df` according to the character-level\n",
    "    # VSM you created above.\n",
    "    \n",
    "    clr = [] # character level representation\n",
    "    for w, _ in df.iterrows():\n",
    "        clr.append(vsm.character_level_rep(w, cf, n))\n",
    "    clr = np.array(clr)\n",
    "\n",
    "    # 3. For each representation created at step 2, add in its\n",
    "    # original representation from `df`. (This should use\n",
    "    # element-wise addition; the dimensionality of the vectors\n",
    "    # will be unchanged.)\n",
    "\n",
    "    # subword enrichment :swe\n",
    "    swe = df.to_numpy() + clr\n",
    "\n",
    "\n",
    "    # 4. Return a `pd.DataFrame` with the same index and column\n",
    "    # values as `df`, but filled with the new representations\n",
    "    # created at step 3.\n",
    "\n",
    "    return pd.DataFrame(swe, index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_subword_enrichment(func):\n",
    "    \"\"\"`func` should be an implementation of subword_enrichment as\n",
    "    defined above.\n",
    "    \"\"\"\n",
    "    vocab = [\"ABCD\", \"BCDA\", \"CDAB\", \"DABC\"]\n",
    "    df = pd.DataFrame([\n",
    "        [1, 1, 2, 1],\n",
    "        [3, 4, 2, 4],\n",
    "        [0, 0, 1, 0],\n",
    "        [1, 0, 0, 0]], index=vocab)\n",
    "    expected = pd.DataFrame([\n",
    "        [14, 14, 18, 14],\n",
    "        [22, 26, 18, 26],\n",
    "        [10, 10, 14, 10],\n",
    "        [14, 10, 10, 10]], index=vocab)\n",
    "    new_df = func(df, n=2)\n",
    "    assert np.array_equal(expected.columns, new_df.columns), \\\n",
    "        \"Columns are not the same\"\n",
    "    assert np.array_equal(expected.index, new_df.index), \\\n",
    "        \"Indices are not the same\"\n",
    "    assert np.array_equal(expected.values, new_df.values), \\\n",
    "        \"Co-occurrence values aren't the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_subword_enrichment(subword_enrichment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsim353        -0.102895\n",
       "mturk771          -0.006531\n",
       "simverb3500dev     0.012539\n",
       "simverb3500test    0.006986\n",
       "men                0.064051\n",
       "Macro-average     -0.005170\n",
       "Name: Spearman r, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "giga20 = pd.read_csv(os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "swe_giga20 = subword_enrichment(giga20)\n",
    "display(full_word_similarity_evaluation(swe_giga20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This question asks you to design your own model. You can of course include steps made above (ideally, the above questions informed your system design!), but your model should not be literally identical to any of the above models. Other ideas: retrofitting, autoencoders, GloVe, subword modeling, ... \n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Your code must operate on one or more of the count matrices in `data/vsmdata`. You can choose which subset of them; this is an important design feature of your system. __Other pretrained vectors cannot be introduced__.\n",
    "\n",
    "1. Retrofitting is permitted.\n",
    "\n",
    "1. Your code must be self-contained, so that we can work with your model directly in your homework submission notebook. If your model depends on external data or other resources, please submit a ZIP archive containing these resources along with your submission.\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies. We also ask that you report the best score your system got during development, just to help us understand how systems performed overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 43528.75439453125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10821805996116596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 30335.89013671875"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14847192304761944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 21547.450683593755"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16082345985385121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 18068.280761718755"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16584782210013793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 16539.783447265625"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17275752490216237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 15738.956298828125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18012496625847577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 15258.352539062525"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18592604498960866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 14940.821777343755"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19189856469920447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 14717.021728515625"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19564005223363895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 14551.838867187555"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1985481278953166\n"
     ]
    }
   ],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "#   3) The score achieved by your system in place of MY_NUMBER.\n",
    "#        With no other changes to that line.\n",
    "#        You should report your score as a decimal value <=1.0\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "# My peak score was: MY_NUMBER\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "\n",
    "giga20 = pd.read_csv(os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "x = subword_enrichment(giga20)\n",
    "x = vsm.pmi(x)\n",
    "#x = vsm.lsa(x, k=10)\n",
    "#x = ttest(x) 0.18\n",
    "#display(full_word_similarity_evaluation(x))\n",
    "\n",
    "n_runs = 10\n",
    "from torch_glove import TorchGloVe\n",
    "#X = pd.read_csv(os.path.join(VSM_HOME, \"giga_window5-scaled.csv.gz\"), index_col=0)\n",
    "mod = TorchGloVe(warm_start=True, max_iter=50)\n",
    "results = []\n",
    "for run in range(n_runs):\n",
    "    G = mod.fit(x)\n",
    "    G_lsa = vsm.lsa(G, k=10)\n",
    "    series = full_word_similarity_evaluation(G_lsa)\n",
    "    print(series['Macro-average'])\n",
    "\n",
    "\n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35093707235233207\n"
     ]
    }
   ],
   "source": [
    "series = full_word_similarity_evaluation(G)\n",
    "print(series['Macro-average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSM: retrofitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_retrofitted = os.path.join(PATH_TO_DATA, 'glove6B300d-retrofit-wn.csv.gz')\n",
    "#import utils\n",
    "G_retrofitted = pd.read_csv(os.path.join(PATH_TO_DATA, \"glove6B300d-retrofit-wn.csv.gz\"),\n",
    "                           index_col=0)\n",
    "#glove_dict = utils.glove2dict(os.path.join(PATH_TO_DATA, \"glove6B300d-retrofit-wn.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.046560</td>\n",
       "      <td>0.213180</td>\n",
       "      <td>-0.007436</td>\n",
       "      <td>-0.458540</td>\n",
       "      <td>-0.035639</td>\n",
       "      <td>0.236430</td>\n",
       "      <td>-0.288360</td>\n",
       "      <td>0.215210</td>\n",
       "      <td>-0.134860</td>\n",
       "      <td>-1.6413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013064</td>\n",
       "      <td>-0.296860</td>\n",
       "      <td>-0.079913</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.031549</td>\n",
       "      <td>0.285060</td>\n",
       "      <td>-0.087461</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>-0.209890</td>\n",
       "      <td>0.053913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.255390</td>\n",
       "      <td>-0.257230</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>-0.042688</td>\n",
       "      <td>0.218170</td>\n",
       "      <td>-0.022702</td>\n",
       "      <td>-0.178540</td>\n",
       "      <td>0.107560</td>\n",
       "      <td>0.058936</td>\n",
       "      <td>-1.3854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075968</td>\n",
       "      <td>-0.014359</td>\n",
       "      <td>-0.073794</td>\n",
       "      <td>0.221760</td>\n",
       "      <td>0.146520</td>\n",
       "      <td>0.566860</td>\n",
       "      <td>0.053307</td>\n",
       "      <td>-0.232900</td>\n",
       "      <td>-0.122260</td>\n",
       "      <td>0.354990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.125590</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>0.103060</td>\n",
       "      <td>-0.101230</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>0.136270</td>\n",
       "      <td>-0.107210</td>\n",
       "      <td>0.236970</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>-1.6785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060148</td>\n",
       "      <td>-0.156190</td>\n",
       "      <td>-0.119490</td>\n",
       "      <td>0.234450</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>0.246180</td>\n",
       "      <td>-0.152420</td>\n",
       "      <td>-0.342240</td>\n",
       "      <td>-0.022394</td>\n",
       "      <td>0.136840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.076947</td>\n",
       "      <td>-0.021211</td>\n",
       "      <td>0.212710</td>\n",
       "      <td>-0.722320</td>\n",
       "      <td>-0.139880</td>\n",
       "      <td>-0.122340</td>\n",
       "      <td>-0.175210</td>\n",
       "      <td>0.121370</td>\n",
       "      <td>-0.070866</td>\n",
       "      <td>-1.5721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366730</td>\n",
       "      <td>-0.386030</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.340360</td>\n",
       "      <td>0.478410</td>\n",
       "      <td>0.068617</td>\n",
       "      <td>0.183510</td>\n",
       "      <td>-0.291830</td>\n",
       "      <td>-0.046533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.257560</td>\n",
       "      <td>-0.057132</td>\n",
       "      <td>-0.671900</td>\n",
       "      <td>-0.380820</td>\n",
       "      <td>-0.364210</td>\n",
       "      <td>-0.082155</td>\n",
       "      <td>-0.010955</td>\n",
       "      <td>-0.082047</td>\n",
       "      <td>0.460560</td>\n",
       "      <td>-1.8477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012806</td>\n",
       "      <td>-0.597070</td>\n",
       "      <td>0.317340</td>\n",
       "      <td>-0.252670</td>\n",
       "      <td>0.543840</td>\n",
       "      <td>0.063007</td>\n",
       "      <td>-0.049795</td>\n",
       "      <td>-0.160430</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>-0.070621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "the  0.046560  0.213180 -0.007436 -0.458540 -0.035639  0.236430 -0.288360   \n",
       ",   -0.255390 -0.257230  0.131690 -0.042688  0.218170 -0.022702 -0.178540   \n",
       ".   -0.125590  0.013630  0.103060 -0.101230  0.098128  0.136270 -0.107210   \n",
       "of  -0.076947 -0.021211  0.212710 -0.722320 -0.139880 -0.122340 -0.175210   \n",
       "to  -0.257560 -0.057132 -0.671900 -0.380820 -0.364210 -0.082155 -0.010955   \n",
       "\n",
       "            7         8       9  ...       290       291       292       293  \\\n",
       "the  0.215210 -0.134860 -1.6413  ... -0.013064 -0.296860 -0.079913  0.195000   \n",
       ",    0.107560  0.058936 -1.3854  ...  0.075968 -0.014359 -0.073794  0.221760   \n",
       ".    0.236970  0.328700 -1.6785  ...  0.060148 -0.156190 -0.119490  0.234450   \n",
       "of   0.121370 -0.070866 -1.5721  ... -0.366730 -0.386030  0.302900  0.015747   \n",
       "to  -0.082047  0.460560 -1.8477  ... -0.012806 -0.597070  0.317340 -0.252670   \n",
       "\n",
       "          294       295       296       297       298       299  \n",
       "the  0.031549  0.285060 -0.087461  0.009061 -0.209890  0.053913  \n",
       ",    0.146520  0.566860  0.053307 -0.232900 -0.122260  0.354990  \n",
       ".    0.081367  0.246180 -0.152420 -0.342240 -0.022394  0.136840  \n",
       "of   0.340360  0.478410  0.068617  0.183510 -0.291830 -0.046533  \n",
       "to   0.543840  0.063007 -0.049795 -0.160430  0.046744 -0.070621  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_retrofitted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Word 'misspend' is in the similarity dataset simverb3500dev but not in the DataFrame, making this evaluation ill-defined. Please switch to a DataFrame with an appropriate vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ab0842fc549b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_word_similarity_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_retrofitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-a8d72d2f7dbd>\u001b[0m in \u001b[0;36mfull_word_similarity_evaluation\u001b[0;34m(df, readers, distfunc)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_similarity_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_reader_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Spearman r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1541c2a22678>\u001b[0m in \u001b[0;36mword_similarity_evaluation\u001b[0;34m(reader, df, distfunc)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;34m\"DataFrame, making this evaluation ill-defined. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;34m\"switch to a DataFrame with an appropriate vocabulary.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     format(w, get_reader_name(reader)))\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Word 'misspend' is in the similarity dataset simverb3500dev but not in the DataFrame, making this evaluation ill-defined. Please switch to a DataFrame with an appropriate vocabulary."
     ]
    }
   ],
   "source": [
    "full_word_similarity_evaluation(G_retrofitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release two additional datasets. The announcement will go out on the discussion forum. We will also release reader code for these datasets that you can paste into this notebook. You will evaluate your custom model $M$ (from the previous question) on these new datasets using `full_word_similarity_evaluation`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time.\n",
    "\n",
    "The announcement will include the details on where to submit your entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bake-off assessment code into this cell.\n",
    "# Please do not remove this comment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    # Please enter your code in the scope of the above conditional.\n",
    "    ##### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your \"Macro-average\" value as reported by the code above.\n",
    "# Please enter only a number between 0 and 1 inclusive.\n",
    "# Please do not remove this comment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    # Please enter your score in the scope of the above conditional.\n",
    "    ##### YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
